{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First thing's first: generate a data set in scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines can be used on problem cases where we have an $n$-dimensional feature space. For teaching purposes, however, it is very intuitive to use a 2-dimensional feature space so you can see what exactly is going on when using support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn has excellent data sets generator. one of them is `make_blobs`, another one is `make_moons`. Let's generate four data sets which we'll all analyze using support vector machines.\n",
    "\n",
    "Run the cell below to create  and plot some sample data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title(\"Two blobs\")\n",
    "X, y = make_blobs(n_features = 2, centers = 2, random_state = 123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s=25)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title(\"Two blobs with more noise\")\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=2.8,  random_state = 123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s=25)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title(\"Three blobs\")\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=0.5,  random_state = 123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s=25)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title(\"Two interleaving half circles\")\n",
    "X, y = make_moons(n_samples=100, shuffle = False , noise = 0.3, random_state=123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s=25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Two groups "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Max margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our first plot again. We'll start with this data set and fit a simple linear support vector machine on these data. You can use the scikit-learn function `svm.SVC` to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features = 2, centers = 2, random_state = 123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below:\n",
    "\n",
    "* Import the `svm` module from sklearn\n",
    "* Create a `svc` object (short for \"Support Vector Classifier\")\n",
    "* Fit it to the data we created in the cell above (`X` and `y`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn .... # uncomment and finish this import statement\n",
    "\n",
    "clf = None\n",
    "# clf.fit(None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the first feature (on the horizontal axis) as X1 and the second feature (on the vertical axis) as X2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1= X[:,0]\n",
    "X2= X[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let'x store the minimum and maximum values X1 and X2 operate in. We'll add some slack (1) to the min and max boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision function\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if what we just did makes sense. Have a look at your plot and verify the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a grid. You can do this by using the numpy function `linspace`, which creates a numpy array with evenly spaced numbers over a specified interval. The default of numbers is 50 and we don't need that many, so let's specify `num = 10` for now. You'll see that you need to up this number one we get to the classification of more than 2 groups.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Set each of the following coordinate variables using `np.linspace()`.  \n",
    "    * For x1, pass in the appropriate min and max values, along with the constant, '10'.\n",
    "    * For x2, pass in the appropriate min and max values, along with the constant, '10'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_coord = None\n",
    "x2_coord = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get the coordinates of the decision function. Run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = clf.decision_function(x1x2).reshape(X1_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X1, X2, c = y)\n",
    "# axes = plt.gca()\n",
    "# axes.contour(X1_C, X2_C, df, colors= \"black\", levels= [-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of the support vectors can be found in the `support_vectors_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create your plot again, but with highlighted support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X1, X2, c = y)\n",
    "# axes = plt.gca()\n",
    "# axes.contour(X1_C, X2_C, df, colors= \"black\", levels= [-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "# axes.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], facecolors='blue') \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Soft margin tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example was pretty easy. The 2 \"clusters\" were easily separable by one straight line classifying every single instance correctly. But what if this isn't the case? Let's have a look at the second dataset we had generated:\n",
    "\n",
    "Run the cell below to recreate and plot our second dataset from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=2.8,  random_state = 123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, repeat the entire process from above.  We're doing the exact same thing as we did above, but to a different dataset--feel free to copy and paste the code you wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = None\n",
    "# clf.fit(None, None)\n",
    "\n",
    "X1= None\n",
    "X2= None\n",
    "X1_min, X1_max = None, None\n",
    "X2_min, X2_max = None, None\n",
    "\n",
    "x1_coord = None\n",
    "x2_coord = None\n",
    "\n",
    "X2_C = None\n",
    "X1_C = None\n",
    "\n",
    "x1x2 = None\n",
    "\n",
    "# df = clf.decision_function(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "# plt.scatter(X1, X2, c = y)\n",
    "# axes = plt.gca()\n",
    "# axes.contour(X1_C, X2_C, df, colors= \"black\", levels= [-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, 3 instances are misclassified (1 yellow, 2 purple). The reason for this is that in scikit learn, the svm module automatically allows for slack variables. If we want to make sure we have as few misclassifications as possible, we should set a bigger value for C, the regularization parameter.\n",
    "\n",
    "Now, we run the same code again, except with a different value for the `C` parameter passed in at initialization for our `svc` object.  Run the cell below and see how our decision boundaries change.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=2.8,  random_state = 123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1000000) \n",
    "clf.fit(X, y)\n",
    "# Start Reusable Section\n",
    "X1= X[:,0]\n",
    "X2= X[:,1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 10)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 10)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "df = clf.decision_function(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "plt.scatter(X1, X2, c = y)\n",
    "axes = plt.gca()\n",
    "axes.contour(X1_C, X2_C, df, colors= \"black\", levels= [-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "plt.show()\n",
    "# End Resuable Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More than 2 groups\n",
    "\n",
    "We'll now repeat the same process as above, but on our 3rd dataset.  This dataset contains classes, turning this from a **_Binary Classification_** to a **_Multiclass Classification_** problem.  \n",
    "\n",
    "Run the cell below to recreate and plot the 3rd dataset we created at the beginning of this lab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=0.5,  random_state = 123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll repeat the same process as we did above.  In the cell below:\n",
    "* Create a `SVC` object.  Set the `kernel` to `\"linear\"`, and `C` to `20`.\n",
    "* `fit` the model to to `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = None\n",
    "# clf.fit(None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to plot the decision boundaries for our multiclass dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1= X[:,0]\n",
    "X2= X[:,1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 200)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 200)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "# Z = clf.predict(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "# axes = plt.gca()\n",
    "# axes.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "# plt.scatter(X1, X2, c = y, edgecolors = 'k')\n",
    "# axes.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], facecolors='blue', edgecolors= 'k') \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would probably be nicer to have non-linear decision boundaries here, let's have a look at that! You can also see how your support vectors are changing. \n",
    "\n",
    "Run the cell below.  Notice that the only substantial change is to the `kernel` parameter--here, we have changed it from `linear` to `rbf` (for \"Radial Basis Function\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=0.5,  random_state = 123)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=20) \n",
    "clf.fit(X, y)\n",
    "\n",
    "X1= X[:,0]\n",
    "X2= X[:,1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "Z = clf.predict(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "plt.scatter(X1, X2, c = y, edgecolors = 'k')\n",
    "axes.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], facecolors='blue', edgecolors= 'k') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one more example of using the kernel trick to find non-linear decision boundaries.  Run the cell below to create another sample dataset, fit an SVM using a non-linear kernel, and plot the decision boundaries.  \n",
    "\n",
    "As we did in previous examples, we have highlighted our support vectors in blue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, shuffle = False , noise = 0.3, random_state=123)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=20) \n",
    "clf.fit(X, y)\n",
    "\n",
    "X1= X[:,0]\n",
    "X2= X[:,1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "Z = clf.predict(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "plt.scatter(X1, X2, c = y, edgecolors = 'k')\n",
    "axes.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], facecolors='blue', edgecolors= 'k') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
